{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaAgAla2Q8+mv8nfDh1IED",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimjeongsoo20190147/dacon/blob/main/dacon_test_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MVf3MtPJMn1F"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from types import SimpleNamespace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter\n",
        "\n",
        "config = {\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"epoch\": 3,\n",
        "    \"batch_size\": 16\n",
        "}\n",
        "\n",
        "CFG = SimpleNamespace(**config)"
      ],
      "metadata": {
        "id": "9rUj_cBgNDde"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")"
      ],
      "metadata": {
        "id": "ap4Fn2ssNE7R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
        "model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=len(train_df['분류'].unique())).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ktTNeyzNPDm",
        "outputId": "84945b19-6741-4939-e630-5f69dc1181cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item] if self.labels is not None else -1\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "omdkgTmFNRWy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "# 데이터 준비\n",
        "train_df['제목_키워드'] = train_df['제목'] + ' ' + train_df['키워드']\n",
        "test_df['제목_키워드'] = test_df['제목'] + ' ' + test_df['키워드']\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = {label: i for i, label in enumerate(train_df['분류'].unique())}\n",
        "train_df['label'] = train_df['분류'].map(label_encoder)\n",
        "\n",
        "# 데이터 분할 (train -> train + validation)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['분류'], random_state=42)\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = TextDataset(train_df.제목_키워드.tolist(), train_df.label.tolist(), tokenizer)\n",
        "val_dataset = TextDataset(val_df.제목_키워드.tolist(), val_df.label.tolist(), tokenizer)\n",
        "test_dataset = TextDataset(test_df.제목_키워드.tolist(), None, tokenizer)  # 라벨 없음\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "EHTHyLVgNUxa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵티마이저 및 학습 파라미터 설정\n",
        "optimizer = AdamW(model.parameters(), lr=CFG.learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Av3YoqQcnrs",
        "outputId": "43833719-f89a-4dcf-b64a-7cb9f942df4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "model.train()\n",
        "for epoch in range(CFG.epoch):\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{CFG.epoch}'):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    val_true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validating'):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            val_predictions.extend(preds.cpu().tolist())\n",
        "            val_true_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    # 검증 결과 출력\n",
        "    val_f1 = f1_score(val_true_labels, val_predictions, average='macro')\n",
        "    print(f\"Validation F1 Score: {val_f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB-EaejOcrnD",
        "outputId": "33667c5c-ad92-4ec5-e922-9a7fb7549960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:   3%|▎         | 67/2185 [24:36<12:41:17, 21.57s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "\n",
        "# 테스트 세트 추론\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc='Testing'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        test_predictions.extend(preds.cpu().tolist())\n",
        "\n",
        "# 라벨 디코딩\n",
        "label_decoder = {i: label for label, i in label_encoder.items()}\n",
        "decoded_predictions = [label_decoder[pred] for pred in test_predictions]"
      ],
      "metadata": {
        "id": "XmxSSY0WNbyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Submission\n",
        "\n",
        "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
        "sample_submission[\"분류\"] = decoded_predictions\n",
        "\n",
        "sample_submission.to_csv(\"./baseline_kobert.csv\", encoding='UTF-8-sig', index=False)"
      ],
      "metadata": {
        "id": "pS47qNNfNgcM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}